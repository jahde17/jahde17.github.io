---
layout: post
title:  "Talking to a Computer"
date:   2017-07-10 15:59:20 +0000
---


This one is probably going to be a short post, but it’s something I’ve come across I thought I would talk about. You see, I am an author, which means that using words is a strength of mine. I’ve had it engrained in my head that to define a word, you should not use the word you are defining in the definition. I carried this philosophy over to coding, and boy was that a mistake.

Coding is like learning a new language. I’ve had a few people laugh and scoff when I make that comparison, but they’ve obviously never tried coding because it is the most true analogy I can think of. This language is so hard because you aren’t learning how to speak to another person; you’re learning how to speak to a machine. How can a human and a computer communicate? Well that’s what code is for. 

I still make the same mistake with human to human language as with human to computer language. With humans, defining a word means putting it in a way that the person will understand it without having to already know what the word means. Code doesn’t work that way. When you’re defining something in code, like say a method in Ruby, you can and often times *should* use the method you’re defining in the “definition”. This was such a hard concept for me to grasp. I kept thinking “but why would I put the method I’m defining in the definition? The computer doesn’t even know what this method is yet! How could this way make the computer understand?” 

I guess what you have to realize with code is that computers don’t work like humans, no matter how much we want them to. It becomes really frustrating and hard to understand, but I think that’s because few people are actually willing to accept that the methods of communicating to a computer are nothing like to a person. A person can compute and adapt as it is being spoken to. A computer needs exact instructions to communicate, and it doesn’t adapt to the conversation. If you say something the computer doesn’t expect or have in its instructions, the computer will simply return an error, telling you it has no idea what you’re saying. It can’t interpret anything to be understood. The coder has to specifically set the environment and words that the computer can use and when. 

This is what I usually think about when I hear those discussions about AI’s eventually taking over the world. I think anything is possible, so I won’t write that off completely, but I do think it is quite near impossible. Artificial Intelligence may become a really great, useful thing, but it will never carry the ability to compute and adapt like a human being, so they could never take over the world. Computers are smart - but only because a human made them that way. Computers seem to be almost magical sometimes (at least to me) but they really are just robots with tons of instructions on how to perform the tasks we need. 

My point to all this rambling was to say that if you are currently learning how to code or you want to start, you shouldn’t look at it as a simple skill to learn. It is a very complex, unique language, and most studies say that it takes years for a person to become fluent in a new language. Usually, that person has to be completely immersed in the language to become fluent. That’s how I view coding. No, we can’t go to a special coding country where everyone speaks in strange tags and methods, but we are exposed to computers and coding everywhere we go. Using that knowledge to your benefit can make the learning process for code so much easier. I expect to keep messing up, but mistakes in coding are golden. You wouldn’t get anywhere without those mistakes. I just hope to get it in my brain that coding is talking to a computer - *not* a person. 

